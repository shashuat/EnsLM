{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import LongformerTokenizer, LongformerModel\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.models import load_model\nimport os\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport pandas as pd\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:14:30.827993Z","iopub.execute_input":"2024-12-12T00:14:30.828324Z","iopub.status.idle":"2024-12-12T00:14:49.853135Z","shell.execute_reply.started":"2024-12-12T00:14:30.828294Z","shell.execute_reply":"2024-12-12T00:14:49.851742Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_path = '/kaggle/input/subevent-data/final_data/train_df.pkl'\ntrain_df = pd.read_pickle(train_path)\n\ntest_path = '/kaggle/input/subevent-data/final_data/test_df.pkl'\ntest_df = pd.read_pickle(test_path)\n\n\nprint('shapes after this : ', train_df.shape, test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:14:49.855070Z","iopub.execute_input":"2024-12-12T00:14:49.855644Z","iopub.status.idle":"2024-12-12T00:14:51.802724Z","shell.execute_reply.started":"2024-12-12T00:14:49.855611Z","shell.execute_reply":"2024-12-12T00:14:51.801747Z"}},"outputs":[{"name":"stdout","text":"shapes after this :  (1778619, 6) (443811, 5)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class TweetDataset(Dataset):\n    \"\"\"Custom dataset for tweet data using Longformer\"\"\"\n    def __init__(self, texts, max_length=4096):\n        self.tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n        self.texts = texts\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        # Longformer requires global attention mask\n        global_attention_mask = torch.zeros_like(encoding['attention_mask'])\n        # Set global attention on [CLS] token\n        global_attention_mask[:, 0] = 1\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'global_attention_mask': global_attention_mask.squeeze()\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:14:51.803983Z","iopub.execute_input":"2024-12-12T00:14:51.804289Z","iopub.status.idle":"2024-12-12T00:14:51.811399Z","shell.execute_reply.started":"2024-12-12T00:14:51.804259Z","shell.execute_reply":"2024-12-12T00:14:51.810393Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\ndef get_longformer_embeddings(text, model, tokenizer, device, max_length=4096):\n    \"\"\"Get embeddings using Longformer model\"\"\"\n    encoding = tokenizer(\n        text,\n        truncation=True,\n        max_length=max_length,\n        padding='max_length',\n        return_tensors='pt'\n    )\n    \n    # Create global attention mask\n    global_attention_mask = torch.zeros_like(encoding['attention_mask'])\n    global_attention_mask[:, 0] = 1  # Global attention on [CLS] token\n    \n    # Move everything to device\n    encoding = {k: v.to(device) for k, v in encoding.items()}\n    global_attention_mask = global_attention_mask.to(device)\n    \n    with torch.no_grad():\n        outputs = model(\n            **encoding,\n            global_attention_mask=global_attention_mask\n        )\n        # Get [CLS] token embedding\n        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n    \n    return embeddings[0]  # Return the first (and only) embedding\n\ndef precompute_all_embeddings(grouped_df, batch_size=8, max_length=4096):\n    \"\"\"Precompute Longformer embeddings for all periods\"\"\"\n    print(\"Precomputing Longformer embeddings for all periods...\")\n    \n    # Initialize model and tokenizer\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\").to(device)\n    tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n    \n    model.eval()\n    \n    # Initialize array to store embeddings\n    embeddings = np.zeros((len(grouped_df), 768))  # 768 is Longformer base embedding dim\n    \n    for idx, row in enumerate(grouped_df.itertuples()):\n        if idx % 100 == 0:\n            print(f\"Processing period {idx}/{len(grouped_df)}\")\n        \n        embeddings[idx] = get_longformer_embeddings(\n            row.Tweet,\n            model,\n            tokenizer,\n            device,\n            max_length=max_length\n        )\n    \n    return embeddings\n\ndef preprocess_data(df, is_training=True):\n    \"\"\"Preprocess the data by grouping tweets by period\"\"\"\n    # Initialize aggregation dictionary\n    agg_dict = {'Tweet': lambda x: ' '.join(x)}\n    \n    if 'EventType' in df.columns and is_training:\n        agg_dict['EventType'] = 'first'\n    if 'MatchID' in df.columns:\n        agg_dict['MatchID'] = 'first'\n    if 'PeriodID' in df.columns:\n        agg_dict['PeriodID'] = 'first'\n        \n    # Group tweets by ID\n    grouped_df = df.groupby('ID').agg(agg_dict).reset_index()\n    \n    # Calculate tweet statistics per period\n    tweet_stats = df.groupby('ID').agg({\n        'Tweet': ['count', lambda x: np.mean([len(t.split()) for t in x])],\n        'MatchID': 'first',\n        'PeriodID': 'first'\n    }).reset_index()\n    \n    tweet_stats.columns = ['ID', 'tweet_count', 'avg_tweet_length', 'MatchID', 'PeriodID']\n    \n    return grouped_df, tweet_stats\n\ndef create_attention_model(input_dim, hidden_dims=[2048, 512, 128, 64]):\n    \"\"\"Create neural network model with additional attention layer\"\"\"\n    # Input layer\n    inputs = layers.Input(shape=(input_dim,))\n    \n    # Reshape inputs to add sequence dimension\n    x = layers.Reshape((input_dim // 64, 64))(inputs)\n    \n    # Additional attention layer\n    attention = layers.Dense(1, use_bias=False)(x)\n    attention = layers.Flatten()(attention)\n    attention_weights = layers.Activation('softmax')(attention)\n    attention_weights = layers.RepeatVector(64)(attention_weights)\n    attention_weights = layers.Permute([2, 1])(attention_weights)\n    \n    # Apply attention weights\n    attended = layers.Multiply()([x, attention_weights])\n    attended = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(attended)\n    \n    # Dense layers with regularization\n    for dim in hidden_dims:\n        attended = layers.Dense(dim, activation='relu')(attended)\n        attended = layers.BatchNormalization()(attended)\n        attended = layers.Dropout(0.5)(attended)\n    \n    outputs = layers.Dense(1, activation='sigmoid')(attended)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\ndef get_callbacks(model_prefix):\n    \"\"\"Create callbacks for training\"\"\"\n    callbacks = [\n        EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_accuracy',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-7,\n            verbose=1\n        ),\n        ModelCheckpoint(\n            f'models_roberta_no_att/{model_prefix}_best_model.keras',\n            monitor='val_accuracy',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    return callbacks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:14:51.813074Z","iopub.execute_input":"2024-12-12T00:14:51.813597Z","iopub.status.idle":"2024-12-12T00:14:51.831749Z","shell.execute_reply.started":"2024-12-12T00:14:51.813553Z","shell.execute_reply":"2024-12-12T00:14:51.830858Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\ndef train_models(train_df, n_splits=5, epochs=30, batch_size=32, max_length=4096, use_attention=True):\n    \"\"\"Train models using cross-validation based on MatchID\"\"\"\n    # Create models directory\n    os.makedirs('models/longformer', exist_ok=True)\n    \n    # Preprocess all data first\n    grouped_df, tweet_stats = preprocess_data(train_df)\n    \n    # Precompute all embeddings once\n    print(\"Precomputing embeddings for all data...\")\n    all_embeddings = precompute_all_embeddings(grouped_df, batch_size=batch_size, max_length=max_length)\n    \n    # Get unique MatchIDs\n    unique_matches = train_df['MatchID'].unique()\n    \n    # Initialize KFold\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    # Initialize lists to store results\n    models = []\n    histories = []\n    all_predictions = []\n    fold_data = []\n    \n    # Train models\n    for fold, (train_idx, val_idx) in enumerate(kf.split(unique_matches)):\n        print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n        \n        # Split data based on MatchID\n        train_matches = unique_matches[train_idx]\n        val_matches = unique_matches[val_idx]\n        \n        # Get training and validation data indices\n        train_mask = grouped_df['MatchID'].isin(train_matches)\n        val_mask = grouped_df['MatchID'].isin(val_matches)\n        \n        # Use pre-computed embeddings\n        X_train = all_embeddings[train_mask]\n        X_val = all_embeddings[val_mask]\n        \n        y_train = grouped_df[train_mask]['EventType'].values\n        y_val = grouped_df[val_mask]['EventType'].values\n        \n        # Create and compile model\n        if use_attention:\n            model = create_attention_model(input_dim=X_train.shape[1])\n        else:\n            print(\"NO\")\n            \n        model.compile(\n            optimizer='adam',\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Train model\n        history = model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            epochs=epochs,\n            batch_size=batch_size,\n            callbacks=get_callbacks(f'fold_{fold}'),\n            verbose=1\n        )\n        \n        # Get predictions for this fold\n        val_preds = model.predict(X_val)\n        fold_predictions = pd.DataFrame({\n            'ID': grouped_df[val_mask]['ID'].values,\n            'fold_pred': val_preds.flatten(),\n            'true_label': y_val,\n            'fold': fold\n        })\n        \n        # Store results\n        models.append(model)\n        histories.append(history.history)\n        all_predictions.append(fold_predictions)\n        fold_data.append({\n            'train_indices': train_idx,\n            'val_indices': val_idx,\n            'train_mask': train_mask,\n            'val_mask': val_mask\n        })\n    \n    # Combine all predictions\n    train_preds = pd.concat(all_predictions, axis=0).reset_index(drop=True)\n    \n    return {\n        'models': models,\n        'histories': histories,\n        'train_preds': train_preds,\n        'grouped_df': grouped_df,\n        'tweet_stats': tweet_stats,\n        'fold_data': fold_data,\n        'embeddings': all_embeddings\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:14:51.834441Z","iopub.execute_input":"2024-12-12T00:14:51.834801Z","iopub.status.idle":"2024-12-12T00:14:51.848339Z","shell.execute_reply.started":"2024-12-12T00:14:51.834772Z","shell.execute_reply":"2024-12-12T00:14:51.847511Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def generate_predictions(df, embeddings, models):\n    \"\"\"Generate predictions from all models for given data\"\"\"\n    predictions = []\n    \n    # Generate predictions from each model\n    for fold, model in enumerate(models):\n        print(f\"Generating predictions for fold {fold}\")\n        fold_preds = model.predict(embeddings)\n        \n        pred_df = pd.DataFrame({\n            'ID': df['ID'],\n            f'fold_{fold}_pred': fold_preds.flatten()\n        })\n        predictions.append(pred_df)\n    \n    # Merge all fold predictions\n    final_preds = predictions[0]\n    for pred_df in predictions[1:]:\n        final_preds = final_preds.merge(pred_df, on='ID')\n    \n    # Calculate mean prediction across folds\n    pred_columns = [col for col in final_preds.columns if 'pred' in col]\n    final_preds['mean_pred'] = final_preds[pred_columns].mean(axis=1)\n    \n    return final_preds\n\ndef create_full_predictions(trained_models, train_df, test_df, load_embeddings=True, embeddings_path='longformer_embeddings_train.npy'):\n    \"\"\"Create comprehensive predictions for both train and test data using Longformer embeddings\"\"\"\n    # Use provided trained models\n    models = trained_models\n    if not models:\n        raise ValueError(\"No models provided!\")\n    \n    # Process train data\n    print(\"Processing training data...\")\n    train_grouped, _ = preprocess_data(train_df, is_training=True)\n    \n    # Process test data\n    print(\"Processing test data...\")\n    test_grouped, _ = preprocess_data(test_df, is_training=False)\n    \n    # Load or compute embeddings\n    if load_embeddings and os.path.exists(embeddings_path):\n        print(\"Loading pre-computed Longformer embeddings...\")\n        all_embeddings = np.load(embeddings_path)\n        train_embeddings = all_embeddings[:len(train_grouped)]\n        if len(test_grouped) > 0:\n            test_embeddings_path = embeddings_path.replace('.npy', '_test.npy')\n            if os.path.exists(test_embeddings_path):\n                test_embeddings = np.load(test_embeddings_path)\n            else:\n                print(\"Computing test embeddings with Longformer...\")\n                test_embeddings = precompute_all_embeddings(test_grouped, max_length=4096)\n                np.save(test_embeddings_path, test_embeddings)\n    else:\n        print(\"Computing embeddings with Longformer...\")\n        train_embeddings = precompute_all_embeddings(train_grouped, max_length=4096)\n        np.save('longformer_embeddings_train.npy', train_embeddings)\n        if len(test_grouped) > 0:\n            test_embeddings = precompute_all_embeddings(test_grouped, max_length=4096)\n            np.save('longformer_embeddings_test.npy', test_embeddings)\n    \n    # Generate predictions\n    print(\"Generating training predictions...\")\n    train_predictions = generate_predictions(train_grouped, train_embeddings, models)\n    \n    # Add true labels to training predictions\n    train_predictions = train_predictions.merge(\n        train_grouped[['ID', 'EventType']], \n        on='ID', \n        how='left'\n    )\n    \n    # Generate test predictions if test data exists\n    test_predictions = None\n    if len(test_grouped) > 0:\n        print(\"Generating test predictions...\")\n        test_predictions = generate_predictions(test_grouped, test_embeddings, models)\n    \n    return {\n        'train_predictions': train_predictions,\n        'test_predictions': test_predictions,\n        'train_grouped': train_grouped,\n        'test_grouped': test_grouped\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:14:51.849482Z","iopub.execute_input":"2024-12-12T00:14:51.849890Z","iopub.status.idle":"2024-12-12T00:14:51.869373Z","shell.execute_reply.started":"2024-12-12T00:14:51.849827Z","shell.execute_reply":"2024-12-12T00:14:51.868502Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"results = train_models(train_df, n_splits=5, epochs=30, batch_size=8, max_length=4096, use_attention=True)\n\n# Access the trained models\nmodels = results['models']\ntrain_val_preds = results['train_preds']\ngrouped_df = results['grouped_df']\ntweet_stats = results['tweet_stats']\nlongformer_embeddings = results['embeddings']\nnp.save('longformer_embeddings_train.npy', longformer_embeddings)\n\n# Generate all predictions\nprediction_results = create_full_predictions(\n    trained_models=models,\n    train_df=train_df,\n    test_df=test_df\n)\n\n# Access predictions\ntrain_preds = prediction_results['train_predictions']\ntest_preds = prediction_results['test_predictions']\n\n# Save predictions\ntrain_preds.to_csv('train_predictions.csv', index=False)\ntest_preds.to_csv('test_predictions.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:14:51.870561Z","iopub.execute_input":"2024-12-12T00:14:51.870988Z","iopub.status.idle":"2024-12-12T00:29:55.393361Z","shell.execute_reply.started":"2024-12-12T00:14:51.870948Z","shell.execute_reply":"2024-12-12T00:29:55.392456Z"}},"outputs":[{"name":"stdout","text":"Precomputing embeddings for all data...\nPrecomputing Longformer embeddings for all periods...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"755dfa70d747456fb60637b8881bdc65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701940e68dec43dcb1923bc0835c147e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ff58eff50c45a9adcebcaf4e785066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c69449f95aa4487dba3565f797d6efaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823dfcdf37d7494088b6bcdd313af8c9"}},"metadata":{}},{"name":"stdout","text":"Processing period 0/2047\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"342685f3967e45d69b456b0e853323c8"}},"metadata":{}},{"name":"stdout","text":"Processing period 100/2047\nProcessing period 200/2047\nProcessing period 300/2047\nProcessing period 400/2047\nProcessing period 500/2047\nProcessing period 600/2047\nProcessing period 700/2047\nProcessing period 800/2047\nProcessing period 900/2047\nProcessing period 1000/2047\nProcessing period 1100/2047\nProcessing period 1200/2047\nProcessing period 1300/2047\nProcessing period 1400/2047\nProcessing period 1500/2047\nProcessing period 1600/2047\nProcessing period 1700/2047\nProcessing period 1800/2047\nProcessing period 1900/2047\nProcessing period 2000/2047\n\nTraining fold 1/5\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1733963081.794106      88 service.cc:145] XLA service 0x79b86c013740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1733963081.794170      88 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 70/191\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5235 - loss: 0.9439","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1733963089.399457      88 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5454 - loss: 0.8971\nEpoch 1: val_accuracy improved from -inf to 0.46731, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 42ms/step - accuracy: 0.5455 - loss: 0.8969 - val_accuracy: 0.4673 - val_loss: 0.6979 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m181/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5712 - loss: 0.7948\nEpoch 2: val_accuracy improved from 0.46731 to 0.61346, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5724 - loss: 0.7920 - val_accuracy: 0.6135 - val_loss: 0.6707 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m187/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5933 - loss: 0.7032\nEpoch 3: val_accuracy improved from 0.61346 to 0.61923, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5930 - loss: 0.7037 - val_accuracy: 0.6192 - val_loss: 0.6449 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m183/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6057 - loss: 0.6916\nEpoch 4: val_accuracy did not improve from 0.61923\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6061 - loss: 0.6906 - val_accuracy: 0.6192 - val_loss: 0.6405 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m176/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6201 - loss: 0.6683\nEpoch 5: val_accuracy did not improve from 0.61923\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6209 - loss: 0.6666 - val_accuracy: 0.6154 - val_loss: 0.6800 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m176/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6390 - loss: 0.6196\nEpoch 6: val_accuracy improved from 0.61923 to 0.63077, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6395 - loss: 0.6196 - val_accuracy: 0.6308 - val_loss: 0.6302 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m168/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6517 - loss: 0.6244\nEpoch 7: val_accuracy did not improve from 0.63077\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6515 - loss: 0.6243 - val_accuracy: 0.6173 - val_loss: 0.6379 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m168/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6628 - loss: 0.6184\nEpoch 8: val_accuracy did not improve from 0.63077\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6634 - loss: 0.6179 - val_accuracy: 0.6288 - val_loss: 0.6882 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6462 - loss: 0.6256\nEpoch 9: val_accuracy did not improve from 0.63077\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6463 - loss: 0.6256 - val_accuracy: 0.6173 - val_loss: 0.6568 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m190/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6791 - loss: 0.6079\nEpoch 10: val_accuracy did not improve from 0.63077\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6792 - loss: 0.6077 - val_accuracy: 0.6154 - val_loss: 0.6355 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m181/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6465 - loss: 0.6005\nEpoch 11: val_accuracy improved from 0.63077 to 0.63654, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6477 - loss: 0.6005 - val_accuracy: 0.6365 - val_loss: 0.6656 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m184/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6828 - loss: 0.6006\nEpoch 12: val_accuracy did not improve from 0.63654\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6826 - loss: 0.6005 - val_accuracy: 0.6231 - val_loss: 0.7115 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m185/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6865 - loss: 0.5765\nEpoch 13: val_accuracy improved from 0.63654 to 0.64038, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6864 - loss: 0.5770 - val_accuracy: 0.6404 - val_loss: 0.6241 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m168/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6535 - loss: 0.6076\nEpoch 14: val_accuracy did not improve from 0.64038\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6551 - loss: 0.6065 - val_accuracy: 0.5788 - val_loss: 0.7400 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m178/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6363 - loss: 0.6184\nEpoch 15: val_accuracy improved from 0.64038 to 0.65000, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6391 - loss: 0.6169 - val_accuracy: 0.6500 - val_loss: 0.6443 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m176/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6909 - loss: 0.5737\nEpoch 16: val_accuracy improved from 0.65000 to 0.66154, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6907 - loss: 0.5743 - val_accuracy: 0.6615 - val_loss: 0.6414 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m179/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6901 - loss: 0.5801\nEpoch 17: val_accuracy did not improve from 0.66154\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6899 - loss: 0.5799 - val_accuracy: 0.6308 - val_loss: 0.6689 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m188/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6965 - loss: 0.5907\nEpoch 18: val_accuracy did not improve from 0.66154\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6968 - loss: 0.5903 - val_accuracy: 0.6000 - val_loss: 0.8067 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m190/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7012 - loss: 0.6037\nEpoch 19: val_accuracy did not improve from 0.66154\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7012 - loss: 0.6035 - val_accuracy: 0.6173 - val_loss: 0.6841 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m189/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6874 - loss: 0.5981\nEpoch 20: val_accuracy did not improve from 0.66154\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6875 - loss: 0.5980 - val_accuracy: 0.5558 - val_loss: 0.8715 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m184/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7114 - loss: 0.5648\nEpoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 21: val_accuracy did not improve from 0.66154\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7111 - loss: 0.5653 - val_accuracy: 0.6115 - val_loss: 0.6820 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m184/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7065 - loss: 0.5630\nEpoch 22: val_accuracy did not improve from 0.66154\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7062 - loss: 0.5632 - val_accuracy: 0.6577 - val_loss: 0.6288 - learning_rate: 5.0000e-04\nEpoch 23/30\n\u001b[1m189/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7070 - loss: 0.5464\nEpoch 23: val_accuracy improved from 0.66154 to 0.69423, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7068 - loss: 0.5468 - val_accuracy: 0.6942 - val_loss: 0.6162 - learning_rate: 5.0000e-04\nEpoch 24/30\n\u001b[1m168/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7049 - loss: 0.5772\nEpoch 24: val_accuracy did not improve from 0.69423\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7042 - loss: 0.5775 - val_accuracy: 0.6635 - val_loss: 0.6743 - learning_rate: 5.0000e-04\nEpoch 25/30\n\u001b[1m168/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7212 - loss: 0.5613\nEpoch 25: val_accuracy improved from 0.69423 to 0.69615, saving model to models_roberta_no_att/fold_0_best_model.keras\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7198 - loss: 0.5608 - val_accuracy: 0.6962 - val_loss: 0.6279 - learning_rate: 5.0000e-04\nEpoch 26/30\n\u001b[1m180/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7081 - loss: 0.5582\nEpoch 26: val_accuracy did not improve from 0.69615\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7080 - loss: 0.5585 - val_accuracy: 0.6692 - val_loss: 0.6563 - learning_rate: 5.0000e-04\nEpoch 27/30\n\u001b[1m180/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7123 - loss: 0.5561\nEpoch 27: val_accuracy did not improve from 0.69615\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7121 - loss: 0.5559 - val_accuracy: 0.6904 - val_loss: 0.6260 - learning_rate: 5.0000e-04\nEpoch 28/30\n\u001b[1m190/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7238 - loss: 0.5428\nEpoch 28: val_accuracy did not improve from 0.69615\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7238 - loss: 0.5430 - val_accuracy: 0.6558 - val_loss: 0.6211 - learning_rate: 5.0000e-04\nEpoch 29/30\n\u001b[1m170/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7061 - loss: 0.5677\nEpoch 29: val_accuracy did not improve from 0.69615\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7066 - loss: 0.5676 - val_accuracy: 0.6654 - val_loss: 0.6237 - learning_rate: 5.0000e-04\nEpoch 30/30\n\u001b[1m169/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6870 - loss: 0.5625\nEpoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 30: val_accuracy did not improve from 0.69615\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6885 - loss: 0.5624 - val_accuracy: 0.6385 - val_loss: 0.6430 - learning_rate: 5.0000e-04\nRestoring model weights from the end of the best epoch: 25.\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n\nTraining fold 2/5\nEpoch 1/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5299 - loss: 0.9792\nEpoch 1: val_accuracy improved from -inf to 0.41795, saving model to models_roberta_no_att/fold_1_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.5300 - loss: 0.9789 - val_accuracy: 0.4179 - val_loss: 0.7245 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m186/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5760 - loss: 0.7836\nEpoch 2: val_accuracy improved from 0.41795 to 0.47692, saving model to models_roberta_no_att/fold_1_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5779 - loss: 0.7819 - val_accuracy: 0.4769 - val_loss: 0.7060 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6223 - loss: 0.7205\nEpoch 3: val_accuracy improved from 0.47692 to 0.61795, saving model to models_roberta_no_att/fold_1_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6222 - loss: 0.7205 - val_accuracy: 0.6179 - val_loss: 0.6724 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m203/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5861 - loss: 0.7168\nEpoch 4: val_accuracy did not improve from 0.61795\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5867 - loss: 0.7163 - val_accuracy: 0.5205 - val_loss: 0.7721 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m192/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6213 - loss: 0.6696\nEpoch 5: val_accuracy improved from 0.61795 to 0.63077, saving model to models_roberta_no_att/fold_1_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6217 - loss: 0.6693 - val_accuracy: 0.6308 - val_loss: 0.6092 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m191/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6292 - loss: 0.6590\nEpoch 6: val_accuracy did not improve from 0.63077\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6293 - loss: 0.6586 - val_accuracy: 0.5821 - val_loss: 0.6854 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m185/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6201 - loss: 0.6543\nEpoch 7: val_accuracy did not improve from 0.63077\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6205 - loss: 0.6539 - val_accuracy: 0.6128 - val_loss: 0.6675 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m202/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6758 - loss: 0.6084\nEpoch 8: val_accuracy did not improve from 0.63077\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6758 - loss: 0.6084 - val_accuracy: 0.6103 - val_loss: 0.6252 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m200/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7029 - loss: 0.5774\nEpoch 9: val_accuracy did not improve from 0.63077\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7023 - loss: 0.5779 - val_accuracy: 0.6128 - val_loss: 0.7936 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m207/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7026 - loss: 0.5687\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 10: val_accuracy did not improve from 0.63077\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7025 - loss: 0.5689 - val_accuracy: 0.6205 - val_loss: 0.6428 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m207/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6956 - loss: 0.5756\nEpoch 11: val_accuracy improved from 0.63077 to 0.63333, saving model to models_roberta_no_att/fold_1_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6956 - loss: 0.5756 - val_accuracy: 0.6333 - val_loss: 0.6239 - learning_rate: 5.0000e-04\nEpoch 12/30\n\u001b[1m188/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6832 - loss: 0.5955\nEpoch 12: val_accuracy improved from 0.63333 to 0.64103, saving model to models_roberta_no_att/fold_1_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6848 - loss: 0.5935 - val_accuracy: 0.6410 - val_loss: 0.6229 - learning_rate: 5.0000e-04\nEpoch 13/30\n\u001b[1m187/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7055 - loss: 0.5812\nEpoch 13: val_accuracy did not improve from 0.64103\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7050 - loss: 0.5794 - val_accuracy: 0.6282 - val_loss: 0.6420 - learning_rate: 5.0000e-04\nEpoch 14/30\n\u001b[1m197/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7162 - loss: 0.5641\nEpoch 14: val_accuracy did not improve from 0.64103\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7158 - loss: 0.5646 - val_accuracy: 0.6256 - val_loss: 0.7070 - learning_rate: 5.0000e-04\nEpoch 15/30\n\u001b[1m197/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7223 - loss: 0.5493\nEpoch 15: val_accuracy improved from 0.64103 to 0.65385, saving model to models_roberta_no_att/fold_1_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7220 - loss: 0.5497 - val_accuracy: 0.6538 - val_loss: 0.6299 - learning_rate: 5.0000e-04\nEpoch 16/30\n\u001b[1m207/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7024 - loss: 0.5497\nEpoch 16: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7025 - loss: 0.5497 - val_accuracy: 0.6103 - val_loss: 0.7312 - learning_rate: 5.0000e-04\nEpoch 17/30\n\u001b[1m206/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7275 - loss: 0.5664\nEpoch 17: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7272 - loss: 0.5666 - val_accuracy: 0.6103 - val_loss: 0.6641 - learning_rate: 5.0000e-04\nEpoch 18/30\n\u001b[1m207/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7323 - loss: 0.5618\nEpoch 18: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7323 - loss: 0.5617 - val_accuracy: 0.6462 - val_loss: 0.6386 - learning_rate: 5.0000e-04\nEpoch 19/30\n\u001b[1m201/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7057 - loss: 0.5797\nEpoch 19: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7060 - loss: 0.5792 - val_accuracy: 0.5821 - val_loss: 1.1149 - learning_rate: 5.0000e-04\nEpoch 20/30\n\u001b[1m201/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7236 - loss: 0.5407\nEpoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 20: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7233 - loss: 0.5413 - val_accuracy: 0.5256 - val_loss: 0.9353 - learning_rate: 5.0000e-04\nEpoch 21/30\n\u001b[1m195/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6936 - loss: 0.5608\nEpoch 21: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6946 - loss: 0.5600 - val_accuracy: 0.6385 - val_loss: 0.6881 - learning_rate: 2.5000e-04\nEpoch 22/30\n\u001b[1m196/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7030 - loss: 0.5627\nEpoch 22: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7042 - loss: 0.5618 - val_accuracy: 0.6462 - val_loss: 0.6409 - learning_rate: 2.5000e-04\nEpoch 23/30\n\u001b[1m192/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7213 - loss: 0.5449\nEpoch 23: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7211 - loss: 0.5461 - val_accuracy: 0.6154 - val_loss: 0.6908 - learning_rate: 2.5000e-04\nEpoch 24/30\n\u001b[1m203/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7100 - loss: 0.5443\nEpoch 24: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7100 - loss: 0.5447 - val_accuracy: 0.6385 - val_loss: 0.7060 - learning_rate: 2.5000e-04\nEpoch 25/30\n\u001b[1m196/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7023 - loss: 0.5672\nEpoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\nEpoch 25: val_accuracy did not improve from 0.65385\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7035 - loss: 0.5656 - val_accuracy: 0.6462 - val_loss: 0.6544 - learning_rate: 2.5000e-04\nEpoch 25: early stopping\nRestoring model weights from the end of the best epoch: 15.\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n\nTraining fold 3/5\nEpoch 1/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5208 - loss: 0.9799\nEpoch 1: val_accuracy improved from -inf to 0.43077, saving model to models_roberta_no_att/fold_2_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5208 - loss: 0.9796 - val_accuracy: 0.4308 - val_loss: 0.7214 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4996 - loss: 0.8498\nEpoch 2: val_accuracy improved from 0.43077 to 0.48462, saving model to models_roberta_no_att/fold_2_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4997 - loss: 0.8497 - val_accuracy: 0.4846 - val_loss: 0.6997 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m205/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5346 - loss: 0.7846\nEpoch 3: val_accuracy improved from 0.48462 to 0.58462, saving model to models_roberta_no_att/fold_2_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5347 - loss: 0.7842 - val_accuracy: 0.5846 - val_loss: 0.6259 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5718 - loss: 0.6995\nEpoch 4: val_accuracy improved from 0.58462 to 0.70769, saving model to models_roberta_no_att/fold_2_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5718 - loss: 0.6995 - val_accuracy: 0.7077 - val_loss: 0.5948 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m202/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5882 - loss: 0.6954\nEpoch 5: val_accuracy did not improve from 0.70769\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5890 - loss: 0.6944 - val_accuracy: 0.6641 - val_loss: 0.6032 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m207/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6251 - loss: 0.6742\nEpoch 6: val_accuracy did not improve from 0.70769\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6251 - loss: 0.6741 - val_accuracy: 0.4769 - val_loss: 0.8176 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m192/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6231 - loss: 0.6533\nEpoch 7: val_accuracy improved from 0.70769 to 0.74872, saving model to models_roberta_no_att/fold_2_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6236 - loss: 0.6532 - val_accuracy: 0.7487 - val_loss: 0.5418 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m194/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6569 - loss: 0.6440\nEpoch 8: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6566 - loss: 0.6437 - val_accuracy: 0.7179 - val_loss: 0.5534 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m201/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6221 - loss: 0.6364\nEpoch 9: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6225 - loss: 0.6362 - val_accuracy: 0.5795 - val_loss: 0.6969 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m202/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6482 - loss: 0.6156\nEpoch 10: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6477 - loss: 0.6159 - val_accuracy: 0.7128 - val_loss: 0.5719 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m202/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6734 - loss: 0.5987\nEpoch 11: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6732 - loss: 0.5991 - val_accuracy: 0.6744 - val_loss: 0.5454 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m206/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6855 - loss: 0.5956\nEpoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 12: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6851 - loss: 0.5959 - val_accuracy: 0.6538 - val_loss: 0.6038 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m206/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7071 - loss: 0.5839\nEpoch 13: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7068 - loss: 0.5841 - val_accuracy: 0.7128 - val_loss: 0.5398 - learning_rate: 5.0000e-04\nEpoch 14/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6725 - loss: 0.6017\nEpoch 14: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6725 - loss: 0.6017 - val_accuracy: 0.5718 - val_loss: 1.2540 - learning_rate: 5.0000e-04\nEpoch 15/30\n\u001b[1m204/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6783 - loss: 0.5984\nEpoch 15: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6780 - loss: 0.5986 - val_accuracy: 0.5744 - val_loss: 0.9725 - learning_rate: 5.0000e-04\nEpoch 16/30\n\u001b[1m198/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6750 - loss: 0.5874\nEpoch 16: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6743 - loss: 0.5879 - val_accuracy: 0.7128 - val_loss: 0.5449 - learning_rate: 5.0000e-04\nEpoch 17/30\n\u001b[1m192/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6788 - loss: 0.5796\nEpoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 17: val_accuracy did not improve from 0.74872\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6784 - loss: 0.5806 - val_accuracy: 0.6487 - val_loss: 0.5736 - learning_rate: 5.0000e-04\nEpoch 17: early stopping\nRestoring model weights from the end of the best epoch: 7.\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n\nTraining fold 4/5\nEpoch 1/30\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5495 - loss: 0.8910\nEpoch 1: val_accuracy improved from -inf to 0.60784, saving model to models_roberta_no_att/fold_3_best_model.keras\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.5496 - loss: 0.8908 - val_accuracy: 0.6078 - val_loss: 0.6686 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5744 - loss: 0.7884\nEpoch 2: val_accuracy improved from 0.60784 to 0.61625, saving model to models_roberta_no_att/fold_3_best_model.keras\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5745 - loss: 0.7882 - val_accuracy: 0.6162 - val_loss: 0.6465 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m194/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6078 - loss: 0.7026\nEpoch 3: val_accuracy improved from 0.61625 to 0.63305, saving model to models_roberta_no_att/fold_3_best_model.keras\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6085 - loss: 0.7027 - val_accuracy: 0.6331 - val_loss: 0.6089 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m200/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6083 - loss: 0.6850\nEpoch 4: val_accuracy did not improve from 0.63305\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6096 - loss: 0.6833 - val_accuracy: 0.5798 - val_loss: 0.6919 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m196/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6202 - loss: 0.6537\nEpoch 5: val_accuracy did not improve from 0.63305\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6201 - loss: 0.6541 - val_accuracy: 0.5294 - val_loss: 0.7291 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m201/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6703 - loss: 0.6072\nEpoch 6: val_accuracy improved from 0.63305 to 0.66667, saving model to models_roberta_no_att/fold_3_best_model.keras\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6698 - loss: 0.6079 - val_accuracy: 0.6667 - val_loss: 0.6075 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m210/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6773 - loss: 0.5911\nEpoch 7: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6770 - loss: 0.5914 - val_accuracy: 0.5378 - val_loss: 0.7481 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6874 - loss: 0.5722\nEpoch 8: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6874 - loss: 0.5723 - val_accuracy: 0.6387 - val_loss: 0.5984 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m207/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6806 - loss: 0.5866\nEpoch 9: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6807 - loss: 0.5867 - val_accuracy: 0.5098 - val_loss: 0.8447 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m207/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6875 - loss: 0.5983\nEpoch 10: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6875 - loss: 0.5982 - val_accuracy: 0.6134 - val_loss: 0.6254 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m208/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6994 - loss: 0.5889\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 11: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6991 - loss: 0.5890 - val_accuracy: 0.6331 - val_loss: 0.6041 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m208/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6844 - loss: 0.5885\nEpoch 12: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6842 - loss: 0.5886 - val_accuracy: 0.5910 - val_loss: 0.7178 - learning_rate: 5.0000e-04\nEpoch 13/30\n\u001b[1m206/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7001 - loss: 0.5704\nEpoch 13: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7002 - loss: 0.5705 - val_accuracy: 0.6050 - val_loss: 0.6842 - learning_rate: 5.0000e-04\nEpoch 14/30\n\u001b[1m193/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6771 - loss: 0.5735\nEpoch 14: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6776 - loss: 0.5741 - val_accuracy: 0.5658 - val_loss: 0.7557 - learning_rate: 5.0000e-04\nEpoch 15/30\n\u001b[1m192/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6819 - loss: 0.5862\nEpoch 15: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6810 - loss: 0.5865 - val_accuracy: 0.5042 - val_loss: 0.9049 - learning_rate: 5.0000e-04\nEpoch 16/30\n\u001b[1m201/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6939 - loss: 0.6014\nEpoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 16: val_accuracy did not improve from 0.66667\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6940 - loss: 0.6003 - val_accuracy: 0.6611 - val_loss: 0.5709 - learning_rate: 5.0000e-04\nEpoch 16: early stopping\nRestoring model weights from the end of the best epoch: 6.\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n\nTraining fold 5/5\nEpoch 1/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5307 - loss: 0.9586\nEpoch 1: val_accuracy improved from -inf to 0.41538, saving model to models_roberta_no_att/fold_4_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5307 - loss: 0.9585 - val_accuracy: 0.4154 - val_loss: 0.7176 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m196/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5625 - loss: 0.7949\nEpoch 2: val_accuracy did not improve from 0.41538\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5628 - loss: 0.7936 - val_accuracy: 0.4154 - val_loss: 0.7187 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m200/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6113 - loss: 0.6960\nEpoch 3: val_accuracy did not improve from 0.41538\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6116 - loss: 0.6959 - val_accuracy: 0.4154 - val_loss: 1.3014 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6179 - loss: 0.6656\nEpoch 4: val_accuracy improved from 0.41538 to 0.41795, saving model to models_roberta_no_att/fold_4_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6179 - loss: 0.6657 - val_accuracy: 0.4179 - val_loss: 1.0683 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m186/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6042 - loss: 0.6848\nEpoch 5: val_accuracy improved from 0.41795 to 0.68205, saving model to models_roberta_no_att/fold_4_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6045 - loss: 0.6832 - val_accuracy: 0.6821 - val_loss: 0.6122 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m194/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6417 - loss: 0.6459\nEpoch 6: val_accuracy did not improve from 0.68205\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6417 - loss: 0.6455 - val_accuracy: 0.4769 - val_loss: 0.7245 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m188/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6322 - loss: 0.6490\nEpoch 7: val_accuracy did not improve from 0.68205\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6342 - loss: 0.6469 - val_accuracy: 0.4154 - val_loss: 1.2154 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m204/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6641 - loss: 0.6053\nEpoch 8: val_accuracy did not improve from 0.68205\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6638 - loss: 0.6055 - val_accuracy: 0.6308 - val_loss: 0.7093 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m186/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6463 - loss: 0.6097\nEpoch 9: val_accuracy did not improve from 0.68205\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6480 - loss: 0.6090 - val_accuracy: 0.5974 - val_loss: 0.6551 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7004 - loss: 0.5842\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 10: val_accuracy did not improve from 0.68205\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7003 - loss: 0.5843 - val_accuracy: 0.4154 - val_loss: 1.2226 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m187/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7220 - loss: 0.5748\nEpoch 11: val_accuracy improved from 0.68205 to 0.69231, saving model to models_roberta_no_att/fold_4_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7196 - loss: 0.5762 - val_accuracy: 0.6923 - val_loss: 0.6046 - learning_rate: 5.0000e-04\nEpoch 12/30\n\u001b[1m205/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6664 - loss: 0.5948\nEpoch 12: val_accuracy did not improve from 0.69231\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6667 - loss: 0.5946 - val_accuracy: 0.6333 - val_loss: 0.6521 - learning_rate: 5.0000e-04\nEpoch 13/30\n\u001b[1m204/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7012 - loss: 0.5780\nEpoch 13: val_accuracy did not improve from 0.69231\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7008 - loss: 0.5783 - val_accuracy: 0.4256 - val_loss: 0.9969 - learning_rate: 5.0000e-04\nEpoch 14/30\n\u001b[1m206/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7058 - loss: 0.5672\nEpoch 14: val_accuracy did not improve from 0.69231\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7054 - loss: 0.5675 - val_accuracy: 0.4179 - val_loss: 1.0463 - learning_rate: 5.0000e-04\nEpoch 15/30\n\u001b[1m201/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6887 - loss: 0.5784\nEpoch 15: val_accuracy did not improve from 0.69231\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6885 - loss: 0.5786 - val_accuracy: 0.6282 - val_loss: 0.6473 - learning_rate: 5.0000e-04\nEpoch 16/30\n\u001b[1m204/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6945 - loss: 0.5915\nEpoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 16: val_accuracy did not improve from 0.69231\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6945 - loss: 0.5912 - val_accuracy: 0.4462 - val_loss: 0.8242 - learning_rate: 5.0000e-04\nEpoch 17/30\n\u001b[1m198/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7064 - loss: 0.5599\nEpoch 17: val_accuracy improved from 0.69231 to 0.69487, saving model to models_roberta_no_att/fold_4_best_model.keras\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7063 - loss: 0.5604 - val_accuracy: 0.6949 - val_loss: 0.5972 - learning_rate: 2.5000e-04\nEpoch 18/30\n\u001b[1m197/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6993 - loss: 0.5648\nEpoch 18: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6988 - loss: 0.5653 - val_accuracy: 0.6872 - val_loss: 0.5983 - learning_rate: 2.5000e-04\nEpoch 19/30\n\u001b[1m203/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6979 - loss: 0.5796\nEpoch 19: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6981 - loss: 0.5796 - val_accuracy: 0.5564 - val_loss: 0.7097 - learning_rate: 2.5000e-04\nEpoch 20/30\n\u001b[1m201/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6987 - loss: 0.5728\nEpoch 20: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6987 - loss: 0.5727 - val_accuracy: 0.4846 - val_loss: 0.8082 - learning_rate: 2.5000e-04\nEpoch 21/30\n\u001b[1m205/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7067 - loss: 0.5573\nEpoch 21: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7065 - loss: 0.5575 - val_accuracy: 0.5692 - val_loss: 0.7058 - learning_rate: 2.5000e-04\nEpoch 22/30\n\u001b[1m200/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6717 - loss: 0.5679\nEpoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\nEpoch 22: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6727 - loss: 0.5673 - val_accuracy: 0.6564 - val_loss: 0.6241 - learning_rate: 2.5000e-04\nEpoch 23/30\n\u001b[1m199/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7076 - loss: 0.5707\nEpoch 23: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7075 - loss: 0.5704 - val_accuracy: 0.6590 - val_loss: 0.6242 - learning_rate: 1.2500e-04\nEpoch 24/30\n\u001b[1m192/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6906 - loss: 0.5732\nEpoch 24: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6914 - loss: 0.5725 - val_accuracy: 0.5256 - val_loss: 0.7603 - learning_rate: 1.2500e-04\nEpoch 25/30\n\u001b[1m200/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7165 - loss: 0.5558\nEpoch 25: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7157 - loss: 0.5562 - val_accuracy: 0.5692 - val_loss: 0.7050 - learning_rate: 1.2500e-04\nEpoch 26/30\n\u001b[1m202/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6967 - loss: 0.5377\nEpoch 26: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6968 - loss: 0.5384 - val_accuracy: 0.4769 - val_loss: 0.8868 - learning_rate: 1.2500e-04\nEpoch 27/30\n\u001b[1m202/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7067 - loss: 0.5616\nEpoch 27: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\nEpoch 27: val_accuracy did not improve from 0.69487\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7070 - loss: 0.5616 - val_accuracy: 0.5718 - val_loss: 0.7462 - learning_rate: 1.2500e-04\nEpoch 27: early stopping\nRestoring model weights from the end of the best epoch: 17.\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\nProcessing training data...\nProcessing test data...\nLoading pre-computed Longformer embeddings...\nComputing test embeddings with Longformer...\nPrecomputing Longformer embeddings for all periods...\nProcessing period 0/516\nProcessing period 100/516\nProcessing period 200/516\nProcessing period 300/516\nProcessing period 400/516\nProcessing period 500/516\nGenerating training predictions...\nGenerating predictions for fold 0\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\nGenerating predictions for fold 1\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nGenerating predictions for fold 2\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nGenerating predictions for fold 3\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nGenerating predictions for fold 4\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nGenerating test predictions...\nGenerating predictions for fold 0\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\nGenerating predictions for fold 1\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\nGenerating predictions for fold 2\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\nGenerating predictions for fold 3\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\nGenerating predictions for fold 4\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n","output_type":"stream"}],"execution_count":7}]}