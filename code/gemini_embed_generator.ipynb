{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:35:09.870899Z",
     "iopub.status.busy": "2024-11-21T17:35:09.870583Z",
     "iopub.status.idle": "2024-11-21T17:35:10.914666Z",
     "shell.execute_reply": "2024-11-21T17:35:10.913748Z",
     "shell.execute_reply.started": "2024-11-21T17:35:09.870848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:35:17.750549Z",
     "iopub.status.busy": "2024-11-21T17:35:17.750278Z",
     "iopub.status.idle": "2024-11-21T17:35:17.770966Z",
     "shell.execute_reply": "2024-11-21T17:35:17.770260Z",
     "shell.execute_reply.started": "2024-11-21T17:35:17.750523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_path = 'final_data/train_df.pkl'\n",
    "train_df = pd.read_pickle(train_path)\n",
    "\n",
    "test_path = 'final_data/test_df.pkl'\n",
    "test_df = pd.read_pickle(test_path)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'INSERT_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:35:17.773098Z",
     "iopub.status.busy": "2024-11-21T17:35:17.772801Z",
     "iopub.status.idle": "2024-11-21T17:35:19.738116Z",
     "shell.execute_reply": "2024-11-21T17:35:19.737172Z",
     "shell.execute_reply.started": "2024-11-21T17:35:17.773072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Group by ID, MatchID, and PeriodID\n",
    "grouped_train = train_df.groupby([ \"MatchID\", \"PeriodID\", \"ID\"]).agg(\n",
    "    EventType=(\"EventType\", \"first\"),\n",
    "    Tweets=(\"Tweet\", \" \".join),  # Concatenate tweets\n",
    "    Tweet_Count=(\"Tweet\", \"count\"),          # Count number of tweets\n",
    ").reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:35:20.897875Z",
     "iopub.status.busy": "2024-11-21T17:35:20.897495Z",
     "iopub.status.idle": "2024-11-21T17:35:21.315009Z",
     "shell.execute_reply": "2024-11-21T17:35:21.313721Z",
     "shell.execute_reply.started": "2024-11-21T17:35:20.897848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Group by ID, MatchID, and PeriodID\n",
    "grouped_test = test_df.groupby([ \"MatchID\", \"PeriodID\", \"ID\",]).agg(\n",
    "    # EventType=(\"EventType\", \"first\"),\n",
    "    Tweets=(\"Tweet\", \" \".join),  # Concatenate tweets\n",
    "    Tweet_Count=(\"Tweet\", \"count\"),          # Count number of tweets\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:38:29.585190Z",
     "iopub.status.busy": "2024-11-21T17:38:29.584227Z",
     "iopub.status.idle": "2024-11-21T17:38:29.596122Z",
     "shell.execute_reply": "2024-11-21T17:38:29.595127Z",
     "shell.execute_reply.started": "2024-11-21T17:38:29.585155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract unique MatchIDs\n",
    "unique_match_ids = grouped_train[\"MatchID\"].unique()\n",
    "\n",
    "# Split MatchIDs into train and validation sets\n",
    "train_ids, valid_ids = train_test_split(\n",
    "    unique_match_ids,\n",
    "    test_size=0.1,\n",
    "    random_state=666\n",
    ")\n",
    "\n",
    "# Filter the train and validation datasets\n",
    "train_df = grouped_train[grouped_train[\"MatchID\"].isin(train_ids)].reset_index(drop=True)\n",
    "val_df = grouped_train[grouped_train[\"MatchID\"].isin(valid_ids)].reset_index(drop=True)\n",
    "test_df = grouped_test\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'embedContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from google.api_core import retry\n",
    "\n",
    "def make_embed_text_fn(model):\n",
    "\n",
    "  @retry.Retry(timeout=300.0)\n",
    "  def embed_fn(text: str) -> list[float]:\n",
    "    # Set the task_type to CLASSIFICATION.\n",
    "    embedding = genai.embed_content(model=model,\n",
    "                                    content=text,\n",
    "                                    task_type=\"classification\")\n",
    "    return embedding['embedding']\n",
    "\n",
    "  return embed_fn\n",
    "\n",
    "def create_embeddings(model, df):\n",
    "  df['Embeddings'] = df['Tweets'].progress_apply(make_embed_text_fn(model))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'models/embedding-001'\n",
    "\n",
    "df_train = create_embeddings(model, train_df)\n",
    "df_val = create_embeddings(model, val_df)\n",
    "df_test = create_embeddings(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('gemini_embeds/train_embeds.csv', index=False)\n",
    "df_val.to_csv('gemini_embeds/val_embeds.csv', index=False)\n",
    "df_test.to_csv('gemini_embeds/test_embeds.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10062584,
     "sourceId": 87245,
     "sourceType": "competition"
    },
    {
     "datasetId": 6073135,
     "sourceId": 9951727,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
