{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'final_data/train_df.pkl'\n",
    "train_df = pd.read_pickle(train_path)\n",
    "\n",
    "test_path = 'final_data/test_df.pkl'\n",
    "test_df = pd.read_pickle(test_path)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Custom dataset for tweet data\"\"\"\n",
    "    def __init__(self, texts, max_length=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "def split_long_text(text, max_length=512, tokenizer=None):\n",
    "    \"\"\"Split long text into chunks of approximately max_length tokens\"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n",
    "    \n",
    "    # Tokenize without truncation to get full length\n",
    "    tokens = tokenizer(text, truncation=False, return_tensors='pt')\n",
    "    total_length = tokens['input_ids'].size(1)\n",
    "    \n",
    "    if total_length <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        # Get token length of current word\n",
    "        word_tokens = len(tokenizer.encode(word)) - 2  # subtract 2 for special tokens\n",
    "        \n",
    "        if current_length + word_tokens > max_length:\n",
    "            # Save current chunk and start new one\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_tokens\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_tokens\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_roberta_embeddings_for_long_text(text, model, tokenizer, device, max_length=512, batch_size=8):\n",
    "    \"\"\"Get embeddings for text that might exceed max_length by averaging chunk embeddings\"\"\"\n",
    "    chunks = split_long_text(text, max_length, tokenizer)\n",
    "    \n",
    "    # Create mini-dataset for chunks\n",
    "    chunk_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        encodings = tokenizer(\n",
    "            batch_chunks,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encodings)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            chunk_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all chunk embeddings and take mean\n",
    "    all_chunk_embeddings = np.vstack(chunk_embeddings)\n",
    "    return np.mean(all_chunk_embeddings, axis=0)\n",
    "\n",
    "def precompute_all_embeddings(grouped_df, batch_size=8, max_length=512):\n",
    "    \"\"\"Precompute RoBERTa embeddings for all periods\"\"\"\n",
    "    print(\"Precomputing RoBERTa embeddings for all periods...\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base\").to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize array to store embeddings\n",
    "    embeddings = np.zeros((len(grouped_df), 768))  # 768 is RoBERTa base embedding dim\n",
    "    \n",
    "    for idx, row in enumerate(grouped_df.itertuples()):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing period {idx}/{len(grouped_df)}\")\n",
    "        \n",
    "        embeddings[idx] = get_roberta_embeddings_for_long_text(\n",
    "            row.Tweet,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            max_length=max_length,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def preprocess_data(df, is_training=True):\n",
    "    \"\"\"Preprocess the data by grouping tweets by period\"\"\"\n",
    "    # Initialize aggregation dictionary\n",
    "    agg_dict = {'Tweet': lambda x: ' '.join(x)}\n",
    "    \n",
    "    if 'EventType' in df.columns and is_training:\n",
    "        agg_dict['EventType'] = 'first'\n",
    "    if 'MatchID' in df.columns:\n",
    "        agg_dict['MatchID'] = 'first'\n",
    "    if 'PeriodID' in df.columns:\n",
    "        agg_dict['PeriodID'] = 'first'\n",
    "        \n",
    "    # Group tweets by ID\n",
    "    grouped_df = df.groupby('ID').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Calculate tweet statistics per period\n",
    "    tweet_stats = df.groupby('ID').agg({\n",
    "        'Tweet': ['count', lambda x: np.mean([len(t.split()) for t in x])],\n",
    "        'MatchID': 'first',\n",
    "        'PeriodID': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    tweet_stats.columns = ['ID', 'tweet_count', 'avg_tweet_length', 'MatchID', 'PeriodID']\n",
    "    \n",
    "    return grouped_df, tweet_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_attention_model(input_dim, hidden_dims=[2048, 512, 128, 64]):\n",
    "    \"\"\"Create neural network model with additional attention layer for BERT embeddings\"\"\"\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Reshape inputs to add sequence dimension (treating BERT features as sequence)\n",
    "    x = layers.Reshape((input_dim // 64, 64))(inputs)  # Reshape to (sequence_length, feature_dim)\n",
    "    \n",
    "    # Additional attention layer\n",
    "    attention = layers.Dense(1, use_bias=False)(x)  # (batch, seq_len, 1)\n",
    "    attention = layers.Flatten()(attention)  # (batch, seq_len)\n",
    "    attention_weights = layers.Activation('softmax')(attention)  # (batch, seq_len)\n",
    "    attention_weights = layers.RepeatVector(64)(attention_weights)  # (batch, feature_dim, seq_len)\n",
    "    attention_weights = layers.Permute([2, 1])(attention_weights)  # (batch, seq_len, feature_dim)\n",
    "    \n",
    "    # Apply attention weights\n",
    "    attended = layers.Multiply()([x, attention_weights])\n",
    "    attended = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(attended)\n",
    "    \n",
    "    # Dense layers with regularization\n",
    "    for dim in hidden_dims:\n",
    "        attended = layers.Dense(dim, activation='relu')(attended)\n",
    "        attended = layers.BatchNormalization()(attended)\n",
    "        attended = layers.Dropout(0.5)(attended)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation='sigmoid')(attended)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(input_dim, hidden_dims=[512, 256, 128]):\n",
    "    \"\"\"Create neural network model for classification\"\"\"\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    x = inputs\n",
    "    for dim in hidden_dims:\n",
    "        x = layers.Dense(dim, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_callbacks(model_prefix):\n",
    "    \"\"\"Create callbacks for training\"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_accuracy',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'models_roberta_no_att/{model_prefix}_best_model.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_models(train_df, n_splits=5, epochs=30, batch_size=32, max_length=512, use_attention=True):\n",
    "    \"\"\"Train models using cross-validation based on MatchID\"\"\"\n",
    "    # Create models directory\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Preprocess all data first\n",
    "    grouped_df, tweet_stats = preprocess_data(train_df)\n",
    "    \n",
    "    # Precompute all embeddings once\n",
    "    print(\"Precomputing embeddings for all data...\")\n",
    "    all_embeddings = precompute_all_embeddings(grouped_df, batch_size=batch_size, max_length=max_length)\n",
    "    \n",
    "    # Get unique MatchIDs\n",
    "    unique_matches = train_df['MatchID'].unique()\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    models = []\n",
    "    histories = []\n",
    "    all_predictions = []\n",
    "    fold_data = []\n",
    "    \n",
    "    # Train models\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(unique_matches)):\n",
    "        print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Split data based on MatchID\n",
    "        train_matches = unique_matches[train_idx]\n",
    "        val_matches = unique_matches[val_idx]\n",
    "        \n",
    "        # Get training and validation data indices\n",
    "        train_mask = grouped_df['MatchID'].isin(train_matches)\n",
    "        val_mask = grouped_df['MatchID'].isin(val_matches)\n",
    "        \n",
    "        # Use pre-computed embeddings\n",
    "        X_train = all_embeddings[train_mask]\n",
    "        X_val = all_embeddings[val_mask]\n",
    "        \n",
    "        y_train = grouped_df[train_mask]['EventType'].values\n",
    "        y_val = grouped_df[val_mask]['EventType'].values\n",
    "        \n",
    "        # Create and compile model\n",
    "        if use_attention:\n",
    "            model = create_attention_model(input_dim=X_train.shape[1])\n",
    "        else:\n",
    "            model = create_model(input_dim=X_train.shape[1])\n",
    "            \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=get_callbacks(f'fold_{fold}'),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Get predictions for this fold\n",
    "        val_preds = model.predict(X_val)\n",
    "        fold_predictions = pd.DataFrame({\n",
    "            'ID': grouped_df[val_mask]['ID'].values,\n",
    "            'fold_pred': val_preds.flatten(),\n",
    "            'true_label': y_val,\n",
    "            'fold': fold\n",
    "        })\n",
    "        \n",
    "        # Store results\n",
    "        models.append(model)\n",
    "        histories.append(history.history)\n",
    "        all_predictions.append(fold_predictions)\n",
    "        fold_data.append({\n",
    "            'train_indices': train_idx,\n",
    "            'val_indices': val_idx,\n",
    "            'train_mask': train_mask,\n",
    "            'val_mask': val_mask\n",
    "        })\n",
    "    \n",
    "    # Combine all predictions\n",
    "    train_preds = pd.concat(all_predictions, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'histories': histories,\n",
    "        'train_preds': train_preds,\n",
    "        'grouped_df': grouped_df,\n",
    "        'tweet_stats': tweet_stats,\n",
    "        'fold_data': fold_data,\n",
    "        'embeddings': all_embeddings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and get all intermediary data\n",
    "results = train_models(train_df, n_splits=5, epochs=30, batch_size=8, max_length=512, use_attention=True)\n",
    "\n",
    "# Access different components\n",
    "models = results['models']\n",
    "train_val_preds = results['train_preds']\n",
    "grouped_df = results['grouped_df']\n",
    "tweet_stats = results['tweet_stats']\n",
    "roberta_embeddings = results['embeddings']\n",
    "np.save('embeddings/roberta_embeddings_train.npy', roberta_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(df, embeddings, models):\n",
    "    \"\"\"Generate predictions from all models for given data\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Generate predictions from each model\n",
    "    for fold, model in enumerate(models):\n",
    "        print(f\"Generating predictions for fold {fold}\")\n",
    "        fold_preds = model.predict(embeddings)\n",
    "        \n",
    "        pred_df = pd.DataFrame({\n",
    "            'ID': df['ID'],\n",
    "            f'fold_{fold}_pred': fold_preds.flatten()\n",
    "        })\n",
    "        predictions.append(pred_df)\n",
    "    \n",
    "    # Merge all fold predictions\n",
    "    final_preds = predictions[0]\n",
    "    for pred_df in predictions[1:]:\n",
    "        final_preds = final_preds.merge(pred_df, on='ID')\n",
    "    \n",
    "    # Calculate mean prediction across folds\n",
    "    pred_columns = [col for col in final_preds.columns if 'pred' in col]\n",
    "    final_preds['mean_pred'] = final_preds[pred_columns].mean(axis=1)\n",
    "    \n",
    "    return final_preds\n",
    "\n",
    "def create_full_predictions(trained_models, train_df, test_df, load_embeddings=True, embeddings_path='embeddings/roberta_embeddings_train.npy'):\n",
    "    \"\"\"Create comprehensive predictions for both train and test data\"\"\"\n",
    "    # Load saved models\n",
    "    # models = load_saved_models()\n",
    "    models = trained_models\n",
    "    if not models:\n",
    "        raise ValueError(\"No saved models found!\")\n",
    "    \n",
    "    # Process train data\n",
    "    print(\"Processing training data...\")\n",
    "    train_grouped, _ = preprocess_data(train_df, is_training=True)\n",
    "    \n",
    "    # Process test data\n",
    "    print(\"Processing test data...\")\n",
    "    test_grouped, _ = preprocess_data(test_df, is_training=False)\n",
    "    \n",
    "    # Load or compute embeddings\n",
    "    if load_embeddings and os.path.exists(embeddings_path):\n",
    "        print(\"Loading pre-computed embeddings...\")\n",
    "        all_embeddings = np.load(embeddings_path)\n",
    "        train_embeddings = all_embeddings[:len(train_grouped)]\n",
    "        if len(test_grouped) > 0:\n",
    "            test_embeddings_path = embeddings_path.replace('.npy', '_test.npy')\n",
    "            if os.path.exists(test_embeddings_path):\n",
    "                test_embeddings = np.load(test_embeddings_path)\n",
    "            else:\n",
    "                print(\"Computing test embeddings...\")\n",
    "                test_embeddings = precompute_all_embeddings(test_grouped)\n",
    "                np.save(test_embeddings_path, test_embeddings)\n",
    "    else:\n",
    "        print(\"Computing embeddings...\")\n",
    "        train_embeddings = precompute_all_embeddings(train_grouped)\n",
    "        np.save('roberta_embeddings.npy', train_embeddings)\n",
    "        if len(test_grouped) > 0:\n",
    "            test_embeddings = precompute_all_embeddings(test_grouped)\n",
    "            np.save('roberta_embeddings_test.npy', test_embeddings)\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating training predictions...\")\n",
    "    train_predictions = generate_predictions(train_grouped, train_embeddings, models)\n",
    "    \n",
    "    # Add true labels to training predictions\n",
    "    train_predictions = train_predictions.merge(\n",
    "        train_grouped[['ID', 'EventType']], \n",
    "        on='ID', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Generate test predictions if test data exists\n",
    "    test_predictions = None\n",
    "    if len(test_grouped) > 0:\n",
    "        print(\"Generating test predictions...\")\n",
    "        test_predictions = generate_predictions(test_grouped, test_embeddings, models)\n",
    "    \n",
    "    return {\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_grouped': train_grouped,\n",
    "        'test_grouped': test_grouped\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all predictions\n",
    "results = create_full_predictions(trained_models=models , train_df=train_df, test_df=test_df)\n",
    "\n",
    "# Access predictions\n",
    "train_preds = results['train_predictions']\n",
    "test_preds = results['test_predictions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.to_csv('final_features/512_RoBERTa_attention/train_predictions', index=False)\n",
    "test_preds.to_csv('final_features/512_RoBERTa_attention/train_predictions', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
